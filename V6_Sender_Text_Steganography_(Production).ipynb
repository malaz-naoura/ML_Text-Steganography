{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnmOyNkdaM9Z"
      },
      "source": [
        "# 0) Download Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7806c0df-e9c0-49d9-e17f-991d067736cc",
        "id": "zTXqlm2nQ5jg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import transformers\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from math import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Initialize parameters"
      ],
      "metadata": {
        "id": "Yk9sagx-pJxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG=False\n",
        "\n",
        "\n",
        "SECRET_MESSAGE=\"بمطعم العم\"\n",
        "SECRET_MESSAGE=\"مرحبا كيفك\"\n",
        "SECRET_MESSAGE=\"الموعد عند الساعه السابعه\"\n",
        "\n",
        "\n",
        "from enum import Enum\n",
        "class SourceCoding(Enum):\n",
        "    HUFFMAN = 1\n",
        "    BINARY  = 3\n",
        "\n",
        "class TextGenerationLang(Enum):\n",
        "    Arabic  = 1\n",
        "\n",
        "class ModelName(Enum):\n",
        "    Roberta = 7\n",
        "\n",
        "class StegoPriority(Enum):\n",
        "    Capacity=1\n",
        "    Efficiency=2\n",
        "    Imperceptibility=3\n",
        "\n",
        "top_k=32\n",
        "sourceCodingType=SourceCoding.HUFFMAN\n",
        "TextGenerationLang=TextGenerationLang.Arabic\n",
        "choosenModel=ModelName.Roberta\n",
        "choosenStegoPriority=StegoPriority.Capacity\n",
        "\n",
        "# Discard the first ? words because they were added to control the context and do not have any data hidden inside them.\n",
        "# This is a constant number that should match between sender and receiver before lunch. (Same as length of senderGeneratedText variable in Sender)\n",
        "numberOfContextWordAdded=4"
      ],
      "metadata": {
        "id": "911LVXrhpUHr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM-ADRkYbQiv"
      },
      "source": [
        "# 3) Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You Model"
      ],
      "metadata": {
        "id": "7qZcKk0gK6Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if choosenModel==ModelName.Roberta:\n",
        "  from transformers import pipeline,TFAutoModelForMaskedLM,TFBertForMaskedLM\n",
        "  from transformers import AutoTokenizer\n",
        "\n",
        "  # For example, this model is not good; you should use your trained model instead.\n",
        "  checkpoint=\"xlm-roberta-base\"\n",
        "\n",
        "  model_name=checkpoint\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  def fillArabicMaskWithRoberta(text,top_k=30):\n",
        "    text=text+\" <mask>\"\n",
        "    model = pipeline(task =\"fill-mask\",model=model_name,tokenizer=tokenizer,top_k =top_k,device=0)\n",
        "    return model(text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "48uDaYFWLJqL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGdal5F1mnSV"
      },
      "source": [
        "# 4) Enconding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uHJ6-1cmsyM"
      },
      "source": [
        "## 1) Huffman Coding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R-dG1Ulvm0Wl"
      },
      "outputs": [],
      "source": [
        "# A Huffman Tree Node\n",
        "class Node:\n",
        "    def __init__(self, prob, symbol, left=None, right=None):\n",
        "        # probability of symbol\n",
        "        self.prob = prob\n",
        "\n",
        "        # symbol\n",
        "        self.symbol = symbol\n",
        "\n",
        "        # left node\n",
        "        self.left = left\n",
        "\n",
        "        # right node\n",
        "        self.right = right\n",
        "\n",
        "        # tree direction (0/1)\n",
        "        self.code = ''\n",
        "\n",
        "\"\"\" A helper function to print the codes of symbols by traveling Huffman Tree\"\"\"\n",
        "codes = dict()\n",
        "\n",
        "def Calculate_Codes(node, val=''):\n",
        "    # huffman code for current node\n",
        "    newVal = val + str(node.code)\n",
        "\n",
        "    if(node.left):\n",
        "        Calculate_Codes(node.left, newVal)\n",
        "    if(node.right):\n",
        "        Calculate_Codes(node.right, newVal)\n",
        "\n",
        "    if(not node.left and not node.right):\n",
        "        codes[node.symbol] = newVal\n",
        "\n",
        "    return codes\n",
        "\n",
        "\"\"\" A helper function to calculate the probabilities of symbols in given data\"\"\"\n",
        "def Calculate_Probability(data):\n",
        "    symbols = dict()\n",
        "    for element in data:\n",
        "        if symbols.get(element) == None:\n",
        "            symbols[element] = 1\n",
        "        else:\n",
        "            symbols[element] += 1\n",
        "    return symbols\n",
        "\n",
        "\"\"\" A helper function to obtain the encoded output\"\"\"\n",
        "def Output_Encoded(data, coding):\n",
        "    encoding_output = []\n",
        "    for c in data:\n",
        "      #  print(coding[c], end = '')\n",
        "        encoding_output.append(coding[c])\n",
        "\n",
        "    string = ''.join([str(item) for item in encoding_output])\n",
        "    return string\n",
        "\n",
        "\"\"\" A helper function to calculate the space difference between compressed and non compressed data\"\"\"\n",
        "def Total_Gain(data, coding):\n",
        "    before_compression = len(data) * 8 # total bit space to stor the data before compression\n",
        "    after_compression = 0\n",
        "    symbols = coding.keys()\n",
        "    for symbol in symbols:\n",
        "        count = data.count(symbol)\n",
        "        after_compression += count * len(coding[symbol]) #calculate how many bit is required for that symbol in total\n",
        "    if DEBUG:\n",
        "      print(\"Space usage before compression (in bits):\", before_compression)\n",
        "      print(\"Space usage after compression (in bits):\",  after_compression)\n",
        "\n",
        "def Huffman_Encoding(data):\n",
        "    symbol_with_probs = Calculate_Probability(data)\n",
        "    symbols = symbol_with_probs.keys()\n",
        "    probabilities = symbol_with_probs.values()\n",
        "    if DEBUG:\n",
        "      print(\"symbols: \", symbols)\n",
        "      print(\"probabilities: \", probabilities)\n",
        "\n",
        "\n",
        "    nodes = []\n",
        "    # converting symbols and probabilities into huffman tree nodes\n",
        "    for symbol in symbols:\n",
        "        nodes.append(Node(symbol_with_probs.get(symbol), symbol))\n",
        "\n",
        "    while len(nodes) > 1:\n",
        "        # sort all the nodes in ascending order based on their probability\n",
        "        nodes = sorted(nodes, key=lambda x: x.prob)\n",
        "        # for node in nodes:\n",
        "        #      print(node.symbol, node.prob)\n",
        "\n",
        "        # pick 2 smallest nodes\n",
        "        right = nodes[0]\n",
        "        left = nodes[1]\n",
        "\n",
        "        left.code = 0\n",
        "        right.code = 1\n",
        "\n",
        "        # combine the 2 smallest nodes to create new node\n",
        "        newNode = Node(left.prob+right.prob, left.symbol+right.symbol, left, right)\n",
        "\n",
        "        nodes.remove(left)\n",
        "        nodes.remove(right)\n",
        "        nodes.append(newNode)\n",
        "\n",
        "    huffman_encoding = Calculate_Codes(nodes[0])\n",
        "    if DEBUG:\n",
        "      print(\"symbols with codes\", huffman_encoding)\n",
        "\n",
        "    Total_Gain(data, huffman_encoding)\n",
        "    encoded_output = Output_Encoded(data,huffman_encoding)\n",
        "    return encoded_output, nodes[0],huffman_encoding\n",
        "\n",
        "\n",
        "def Huffman_Decoding(encoded_data, huffman_tree,wordSplit=False):\n",
        "    tree_head = huffman_tree\n",
        "    decoded_output = []\n",
        "    for x in encoded_data:\n",
        "        if x == '1':\n",
        "            huffman_tree = huffman_tree.right\n",
        "        elif x == '0':\n",
        "            huffman_tree = huffman_tree.left\n",
        "        try:\n",
        "            if huffman_tree.left.symbol == None and huffman_tree.right.symbol == None:\n",
        "                pass\n",
        "        except AttributeError:\n",
        "            decoded_output.append(huffman_tree.symbol)\n",
        "            huffman_tree = tree_head\n",
        "\n",
        "    delimiter= ' ' if wordSplit else ''\n",
        "    string = delimiter.join([str(item) for item in decoded_output])\n",
        "    return string\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encodeByHuffman(data,wordSplit=False):\n",
        "  data=data.strip()\n",
        "  if wordSplit:\n",
        "    data=data.split(' ')\n",
        "\n",
        "  global codes\n",
        "  codes=dict()\n",
        "  encoding, tree,mapping = Huffman_Encoding(data)\n",
        "  return (encoding,tree,mapping)\n",
        "\n",
        "def decodeByHuffman(encoding,tree,wordSplit=False):\n",
        "  return Huffman_Decoding(encoding,tree,wordSplit)\n",
        "\n"
      ],
      "metadata": {
        "id": "UiochXCooUeS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Binary Tree Coding"
      ],
      "metadata": {
        "id": "oeeACuajn_75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buildBinaryTreeCoding(words,currCode=''):\n",
        "\n",
        "  l=len(words)\n",
        "\n",
        "  if log2(l)!= log2(pow(2,int(log2(l)))):\n",
        "    print('The Length Is Not Power Of 2')\n",
        "    return None\n",
        "\n",
        "  if l==1:\n",
        "    return [currCode]\n",
        "    # return [(words[0],currCode)]\n",
        "\n",
        "  mid=int(l/2)\n",
        "  a=buildBinaryTreeCoding(words[:mid],currCode+'0')\n",
        "  b=buildBinaryTreeCoding(words[mid:],currCode+'1')\n",
        "\n",
        "  return a+b\n",
        "\n",
        "def binaryTreeEncoding(words):\n",
        "  l=len(words)\n",
        "  leafNb=int(pow(2,int(log2(l))))\n",
        "  restToCeilLog=int(pow(2,ceil(log2(l))))-leafNb\n",
        "  words=words+['NO_TOKEN_'+str(i) for i in range(1,restToCeilLog)]\n",
        "  if DEBUG==True:\n",
        "    print(len(words))\n",
        "    print(words)\n",
        "  wordsCode=buildBinaryTreeCoding(words)\n",
        "  mapWordToCode= {k:v  for k,v in zip(words,wordsCode) }\n",
        "  encoding=''.join(list(map(lambda x:mapWordToCode[x],words)))\n",
        "  return encoding,mapWordToCode\n",
        "\n"
      ],
      "metadata": {
        "id": "V0ZlLLExoXK2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encodeByBinaryTree(data,wordSplit=False):\n",
        "  data=data.strip()\n",
        "  if wordSplit:\n",
        "    data=data.split(' ')\n",
        "  else:\n",
        "    data=list(set(data))\n",
        "\n",
        "  encoding,mapping = binaryTreeEncoding(data)\n",
        "  return (encoding,mapping)\n"
      ],
      "metadata": {
        "id": "JtOXXvpqocaq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ImOw1HwyGS3"
      },
      "source": [
        "# OO ) Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAfPThrO-1to"
      },
      "source": [
        "### Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fillMask(generatedText,top_k=top_k):\n",
        "  top_k=top_k*2\n",
        "  masks=[]\n",
        "  if choosenModel==ModelName.Roberta:\n",
        "    masks=fillArabicMaskWithRoberta(generatedText,top_k=top_k)\n",
        "  return masks"
      ],
      "metadata": {
        "id": "KvareR4TU819"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ax2HIKli-5lK"
      },
      "outputs": [],
      "source": [
        "def generateNextMaskedWord(generatedText,wordIdx,sourceCodingType,top_kth):\n",
        "  tok_k=top_kth\n",
        "\n",
        "  if TextGenerationLang==TextGenerationLang.Arabic:\n",
        "    substitution=r'[^\\u0620-\\u064a ]*'\n",
        "  elif TextGenerationLang==TextGenerationLang.English:\n",
        "    substitution=r'[^a-zA-Z ]*'\n",
        "\n",
        "  masks=fillMask(generatedText,top_k=top_k)\n",
        "  masks=list(map(lambda ele: re.sub(substitution, '', ele[\"sequence\"].split(\" \")[-1]),masks))\n",
        "\n",
        "  if DEBUG==True:\n",
        "    print(\"Masks: \",masks)\n",
        "\n",
        "  lastWord=''\n",
        "  if DEBUG==True:\n",
        "    print(\"last element of genereted text : \" )\n",
        "    print(generatedText.split(\" \"))\n",
        "\n",
        "  if len(generatedText.split(\" \"))>0:\n",
        "    if DEBUG==True:\n",
        "      print(\"last element of genereted text : \"+ generatedText.split(\" \")[-1])\n",
        "    lastWord=generatedText.split(\" \")[-1]\n",
        "  masks=list(filter(lambda ele: (ele!='' and len(ele)!=1 and ele!=lastWord) ,masks))\n",
        "  masks=masks[0:max(len(masks),int(top_k/2))]\n",
        "\n",
        "  encoding=mapWordToCode=None\n",
        "  if sourceCodingType==SourceCoding.HUFFMAN:\n",
        "    masks=' '.join(masks)\n",
        "    encoding,_,mapWordToCode=encodeByHuffman(masks,True)\n",
        "  elif sourceCodingType==SourceCoding.BINARY:\n",
        "    l=len(masks)\n",
        "    leafNb=int(pow(2,int(log2(l))))\n",
        "    if DEBUG==True:\n",
        "      print('leafNb: ',leafNb)\n",
        "    masks=masks[:leafNb]\n",
        "    # if DEBUG==True:\n",
        "    print(\"edit len mask : \",len(masks))\n",
        "    masks=' '.join(masks)\n",
        "    encoding,mapWordToCode=encodeByBinaryTree(masks,True)\n",
        "\n",
        "  mapCodeToWord=dict(map(reversed, mapWordToCode.items()))\n",
        "  return (encoding,mapWordToCode,mapCodeToWord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM7eJShBWGuD"
      },
      "source": [
        "## Sender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MK-CpqAoySB9"
      },
      "outputs": [],
      "source": [
        "SECRET_MESSAGE = re.sub('[^\\u0620-\\u064a ]+', '', SECRET_MESSAGE)\n",
        "mapLetterToCode = {'ل': '0000', 'ص': '000100', 'ج': '0001010', 'ز': '0001011', 'أ': '000110', 'ف': '000111', 'ك': '001000', 'إ': '00100100', 'ظ': '0010010100', 'آ': '0010010101', 'ء': '001001011', 'ش': '0010011', 'ب': '00101', 'و': '00110', 'ح': '0011100', 'خ': '0011101', 'د': '001111',\n",
        "                  'م': '01000', 'ع': '01001', 'ط': '0101000', 'ذ': '0101001', 'ى': '0101010', 'ض': '0101011', 'ر': '01011', 'ي': '0110', 'ة': '011100', 'س': '011101', 'ت': '01111', 'ق': '100000', 'ث': '10000100', 'غ': '10000101', 'ئ': '1000011', 'ه': '10001', 'ن': '1001', 'ا': '101', ' ': '11'}\n",
        "encodingSecretMessage = ''.join(list(map(lambda x: mapLetterToCode[x], SECRET_MESSAGE)))\n",
        "\n",
        "END_TOKEN_CODE = \"0\"*8\n",
        "encodingSecretMessage = encodingSecretMessage+END_TOKEN_CODE\n",
        "if DEBUG==True:\n",
        "  print(encodingSecretMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1XGjpFfOAiix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae47c1f2-3243-4ec1-c55a-556e1346f8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "senderGeneratedText : أعلن الريس على عن حياته قريبا السبت الجمعة الماضي إلى حين تاريخ النهاية مع العدالة المتحدة الجمعة قبل الغد وبعد الأربعاء هذا هذان هذانث هذانثة هذانثةه هو\n"
          ]
        }
      ],
      "source": [
        "# To control the context that the model should commit to, you may add words from a prefix of a phrase by selecting a sentence at random from your list.\n",
        "# However, keep in mind that the sentence's length should equal numberOfContextWordAdded variable\n",
        "senderGeneratedText='أعلن الريس على عن'\n",
        "\n",
        "if DEBUG==True:\n",
        "  print(len(senderGeneratedText.split(' ')))\n",
        "  print(numberOfContextWordAdded)\n",
        "\n",
        "if(len(senderGeneratedText.split(' '))!=numberOfContextWordAdded):\n",
        "  raise RuntimeError(\"numberOfContextWordAdded should equal the length of senderGeneratedText\")\n",
        "\n",
        "wordIdx=0\n",
        "while len(encodingSecretMessage):\n",
        "  encoding,mapWordToCode,mapCodeToWord=generateNextMaskedWord(senderGeneratedText,wordIdx,sourceCodingType,top_k)\n",
        "  wordIdx=wordIdx+1\n",
        "  if DEBUG==True:\n",
        "    print(mapCodeToWord.keys())\n",
        "    print(mapCodeToWord)\n",
        "\n",
        "\n",
        "  encodingSecretMessageLength=len(encodingSecretMessage)\n",
        "  chooseWord=False\n",
        "  for i in range(0,encodingSecretMessageLength):\n",
        "    senderCurrEncodingMatchTest=encodingSecretMessage[0:encodingSecretMessageLength-i]\n",
        "\n",
        "\n",
        "    if mapCodeToWord.get(senderCurrEncodingMatchTest)==None:\n",
        "      continue\n",
        "\n",
        "    senderChoosenWord=mapCodeToWord.get(senderCurrEncodingMatchTest)\n",
        "    if DEBUG==True:\n",
        "      print(senderCurrEncodingMatchTest +\" ----> \"+senderChoosenWord)\n",
        "\n",
        "    if len(senderGeneratedText)!=0:\n",
        "      senderGeneratedText=senderGeneratedText+\" \"\n",
        "    senderGeneratedText=senderGeneratedText+senderChoosenWord\n",
        "    encodingSecretMessage=encodingSecretMessage[encodingSecretMessageLength-i:]\n",
        "    if DEBUG==True:\n",
        "      print(\"rest encoding secrete message : \"+encodingSecretMessage)\n",
        "      print(\"length rest encoding secrete message : \"+str(len(encodingSecretMessage)))\n",
        "    chooseWord=True\n",
        "    break\n",
        "\n",
        "  if DEBUG==True:\n",
        "    print(\"out for loop with choose word value : \"+str(chooseWord))\n",
        "\n",
        "  if chooseWord==False:\n",
        "    keys=mapCodeToWord.keys()\n",
        "    keys=list(keys)\n",
        "    keys.sort(reverse=True)\n",
        "    keys=' '.join(keys)\n",
        "    x = re.search(\"\\s\"+encodingSecretMessage+\".*\\s\", keys)\n",
        "    keys=keys[x.start():]\n",
        "    senderCurrEncodingMatchTest=keys.split(\" \")[1]\n",
        "    if DEBUG==True:\n",
        "      print(\"***************************\")\n",
        "      print(senderCurrEncodingMatchTest)\n",
        "    senderChoosenWord=mapCodeToWord.get(senderCurrEncodingMatchTest)\n",
        "    if len(senderGeneratedText)!=0:\n",
        "     senderGeneratedText=senderGeneratedText+' '\n",
        "    senderGeneratedText=senderGeneratedText+senderChoosenWord\n",
        "    encodingSecretMessage=\"\"\n",
        "\n",
        "    if DEBUG==True:\n",
        "      print(\"***************************\")\n",
        "\n",
        "if DEBUG==True:\n",
        "  print(\"length of secret message enconding : \"+str(len(encodingSecretMessage)))\n",
        "  print(encodingSecretMessage)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"senderGeneratedText : \"+senderGeneratedText)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}